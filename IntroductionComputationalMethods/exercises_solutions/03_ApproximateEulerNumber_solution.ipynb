{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B3AO3vaCDqRy"
   },
   "source": [
    "# Approximation of Euler Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MesPZ18epOHI"
   },
   "source": [
    "In this exercise, you will explore the effect of working with different numeric types on the precision of your results and visualize results using `matplotlib`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vk0r6t8kFQZO"
   },
   "source": [
    "[Euler's number](https://en.wikipedia.org/wiki/E_(mathematical_constant)) *e* can be calculated as the sum of the infinte series \n",
    "\n",
    "$$e=\\sum_{n=0}^\\infty\\frac{1}{n\\,!}\\; .$$\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "1. Approximate Euler's number for different $n$ between $n=0\\ldots30$ using above formula.\n",
    "   \n",
    "   You can use [`scipy.special.factorial`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.factorial.html#scipy.special.factorial) to compute the factorial.\n",
    "  - Compare your approximation for each $n$ with `np.e` by computing the 'error' as the absolute difference between your approximation and `np.e`.\n",
    "  - What is the 'error'? What is your 'best' estimate of $e$.\n",
    "\n",
    "\n",
    "2. Plot your results: \n",
    "  - Plot the result of your approximation (y-axis) against $n$ (x-axis).\n",
    "  - Plot the error (y-axis) against $n$ (x-axis). You may want to use a [logarithmic y-axis](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.semilogy.html#matplotlib.pyplot.semilogy).\n",
    "\n",
    "\n",
    "3. Compare data type precision with error.\n",
    "  - Get the precision of the data type that you are using for computation.\n",
    "    If you are using `numpy` and have not specified the numeric type explicitly, this will be `np.float64`.\n",
    "  - Use  [`finfo`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.finfo.html#numpy-finfo) to obtain the smallest representable positive number with this data type, e.g. `np.finfo(np.float64).eps`\n",
    "  - Compare this *eps* number to the 'error' in your approximation of $e$ for $n>20$.\n",
    "  - Add a horizontal line to your 'error' plot that indicates this *eps* number using [plt.axhline](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.axhline.html).\n",
    "  \n",
    "  \n",
    "4. Compare with the error for 32-bit approximation\n",
    "  - Limit the precision of your approximation to 32-bit and compute error.\n",
    "      \n",
    "    *Note:*\n",
    "    \n",
    "    We don't have full control over the numerical precision used by the [factorial](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.factorial.html#scipy.special.factorial).\n",
    "    Therefore, simply reduce the precision of the $e$ estimates to 32-bit before computing the error between estimated and reference $e$.\n",
    "    You can do this by \n",
    "    ```\n",
    "    value_e_32 = np.array(value_e, dtype=np.float32)\n",
    "    ```\n",
    "    assuming that `value_e` is a list or numpy array containing your estimates of $e$.\n",
    "\n",
    "  - Replot. As before, include the precision of the `np.float32` data type in your plot.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUHTIU2_Nqui"
   },
   "source": [
    "## Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymYLxPKra7X4"
   },
   "outputs": [],
   "source": [
    "# Approximate 'e'\n",
    "import numpy as np\n",
    "from scipy.special import factorial\n",
    "\n",
    "# data type\n",
    "num_type = np.float64\n",
    "# n elements in sum\n",
    "n = 30\n",
    "\n",
    "# array for storing approximation of e for each iteration\n",
    "value_e = np.zeros(n, dtype=num_type)\n",
    "\n",
    "# compute the sum for i = 0 to n\n",
    "e_approx = 0.0\n",
    "for i in range(n):\n",
    "      e_approx = e_approx + 1/factorial(i)\n",
    "      value_e[i] = e_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "I3BiYcEUdana",
    "outputId": "4c6b5f5b-b272-4a9b-ad04-df5bec7b779b"
   },
   "outputs": [],
   "source": [
    "# Compute error as absolute difference to np.e\n",
    "e_ref   = np.ones(n, dtype=np.float64)*np.e  \n",
    "error_e = np.abs(e_ref - value_e)\n",
    "\n",
    "min_err = np.min(error_e)\n",
    "min_iter= np.argmin(error_e)\n",
    "print(\"The smallest error is %.5g at iteration %i:\"%(min_err, min_iter))\n",
    "print(\"The best estimate of 'e' is %.10f.\"%value_e[min_iter])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nf-liAycPIS0"
   },
   "source": [
    "## Plot of Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "UM_nyxnLRCgi",
    "outputId": "4b7ba7dc-8f21-4a9e-f670-3d03e8e967d0"
   },
   "outputs": [],
   "source": [
    "# plot approximated 'e' and actual e\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "n_iter = np.arange(n)\n",
    "\n",
    "plt.plot(n_iter, value_e, label='e (approximated)')\n",
    "plt.axhline(y=np.e, linestyle='--', color='b', label='e')\n",
    "plt.legend(loc='best', frameon=True, facecolor='white', framealpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74Mt1bt5Sz42"
   },
   "source": [
    "## Plot of Error and Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UfvRLvPHRTkL"
   },
   "source": [
    "We obtain the data type of our approximation and the precision of the smallest representable positive number with this data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wIE0F2oFRfY_",
    "outputId": "0abce798-a607-4571-d853-94812427e154"
   },
   "outputs": [],
   "source": [
    "data_type = type(value_e[-1])\n",
    "print(\"The data type is: \", data_type) \n",
    "\n",
    "precision = np.finfo(np.float64).eps\n",
    "print(\"precision: \", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "colab_type": "code",
    "id": "5o6ueROxhlA5",
    "outputId": "df0a6f70-ae2a-44ec-a459-0a6dac358367"
   },
   "outputs": [],
   "source": [
    "# plot approximated 'e', actual e, and error\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "n_iter = np.arange(n)\n",
    "\n",
    "plt.semilogy(n_iter, value_e, label='e (approximated)')\n",
    "plt.axhline(y=np.e, linestyle='--', color='b', label='e')\n",
    "\n",
    "plt.semilogy(n_iter, error_e, label='error')\n",
    "plt.axhline(y=precision, linestyle='--', \n",
    "            color='r', label='precision 64 bit')\n",
    "\n",
    "plt.legend(loc='best', frameon=True, facecolor='white', framealpha=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTDtAWH0S_CI"
   },
   "source": [
    "## Error for 32-bit approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbt_4-eJTFgS"
   },
   "source": [
    "We don't have full control over the numerical precision used by the [factorial](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.factorial.html#scipy.special.factorial).\n",
    "\n",
    "Therefore, we only reduce the precision of the $e$ estimates to 32-bit before computing the error between estimated and reference $e$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "po7g-LxgUSkD",
    "outputId": "d6fa251a-8855-44b2-dbbf-174ddc0dd0bf"
   },
   "outputs": [],
   "source": [
    "value_e_32 = np.array(value_e, dtype=np.float32)\n",
    "error_e_32 = np.abs(np.ones(n, dtype=np.float64) * np.e  - value_e_32)\n",
    "\n",
    "min_err = np.min(error_e_32)\n",
    "min_iter= np.argmin(error_e_32)\n",
    "print(\"The smallest error is %.5g at iteration %i:\"%(min_err, min_iter))\n",
    "print(\"The best estimate of 'e' is %.10f.\"%value_e_32[min_iter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "colab_type": "code",
    "id": "C-XQQGdjVLi8",
    "outputId": "77fbc853-a421-47d4-f9da-4992de5265be"
   },
   "outputs": [],
   "source": [
    "# plot approximated 'e', actual e, and error\n",
    "\n",
    "precision_32 = np.finfo(np.float32).eps\n",
    "\n",
    "n_iter = np.arange(n)\n",
    "\n",
    "plt.semilogy(n_iter, value_e_32, label='e (approximated)')\n",
    "plt.axhline(y=np.e, linestyle='--', color='b', label='e')\n",
    "\n",
    "plt.semilogy(n_iter, error_e_32, label='error')\n",
    "plt.axhline(y=precision, linestyle='--', \n",
    "            color='r', label='precision - 64bit')\n",
    "plt.axhline(y=precision_32, linestyle='--', \n",
    "            color='orange', label='precision - 32 bit')\n",
    "\n",
    "plt.legend(loc='upper right', frameon=True, facecolor='white', framealpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w5EloN9GchqJ"
   },
   "source": [
    "## Alternative computation via 'vectorization'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CKx7l0L3uhlf"
   },
   "source": [
    "Using NumPy, the for-loop can be *vectorized*.\n",
    "Instead of repeatedly performing the same operation on one element of the list/array, we apply the operation to all elements simultaneously, and then use specialised functions that collect the individual components.\n",
    "In Python or Matlab, *vectorization* is often faster than iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23elkVA6oIev"
   },
   "outputs": [],
   "source": [
    "# Approximate 'e' using vectorization\n",
    "\n",
    "n_iter = np.arange(30, dtype=np.float64)   \n",
    "n_factorial = factorial(n_iter)            # applies the factorial function \n",
    "                                           # to each element of the array\n",
    "e_approx_np = np.sum(1./n_factorial)       # computes 1./value for each element \n",
    "                                           # in list and sums them up\n",
    "error_e_np = np.e - e_approx_np\n",
    "error_e_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtMTzg7cdwXO"
   },
   "source": [
    "Consider the following two examples for comparing speed differences between loop-based iterations and vectorized operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AS9P4oJnwJ-R"
   },
   "outputs": [],
   "source": [
    "# Speed comparison: for loop\n",
    "%%timeit\n",
    "x = np.arange(100000)\n",
    "sum_for = 0 \n",
    "for i in x: \n",
    "    sum_for = sum_for + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGdtG7jjxM6Y"
   },
   "outputs": [],
   "source": [
    "# Speed comparison: for vectorized operation\n",
    "%%timeit\n",
    "x = np.arange(100000)\n",
    "sum_vec = x.sum()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "approximate_euler_number_solution.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
